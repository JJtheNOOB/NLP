{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Partitions to 8 (parallelism of 8 / multiple executors)\n",
    "from pyspark.sql import SQLContext, Row\n",
    "\n",
    "tech_text = sc.wholeTextFiles(\"/mnt/dataset/public/bbcnews/tech/\",8).map(lambda (a,b): Row(title =a.replace('dbfs:/mnt/dataset/public/bbcnews/tech/',''), \n",
    "                                                                                           text=b) ).toDF([\"doc\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show\n",
    "display(tech_text.selectExpr(\"text as doc\",\"doc as text\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Compute N (Number Of Documents in Corpus)\n",
    "- This count is used in IDF computation.\n",
    "- We do this first inorder to activate the caching of the RDD above. So that subsequent calls to the RDD can would be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note the parallelism of 8\n",
    "number_of_docs = tech_text.count()\n",
    "number_of_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Compute Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text to words.\n",
    "import re\n",
    "def tokenize(s):\n",
    "    return re.split(\"\\\\W+\", s.lower())\n",
    "\n",
    "\n",
    "tokenized_text = tech_text.map(lambda (text,title): (title, tokenize(text)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#term frequencies in each document\n",
    "term_frequency = tokenized_text.flatMapValues(lambda x: x).countByValue()\n",
    "term_frequency.items()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Compute Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count how many documents a word appears in.\n",
    "document_frequency = tokenized_text.flatMapValues(lambda x: x).distinct()\\\n",
    "                        .filter(lambda x: x[1] != '')\\\n",
    "                        .map(lambda (title,word): (word,title)).countByKey()\n",
    "document_frequency.items()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Compute TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "def tf_idf(N, tf, df):\n",
    "    result = []\n",
    "    for key, value in tf.items():\n",
    "        doc = key[0]\n",
    "        term = key[1]\n",
    "        df = document_frequency[term]\n",
    "        if (df>0):\n",
    "              tf_idf = float(value)*np.log(number_of_docs/df)\n",
    "        \n",
    "        result.append({\"doc\":doc, \"term\":term, \"score\":tf_idf})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_output = tf_idf(number_of_docs, term_frequency, document_frequency)\n",
    "tf_idf_output[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Performing Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The search Funtion\n",
    "\n",
    "tfidf_RDD = sc.parallelize(tf_idf_output).map(lambda x: (x['term'],(x['doc'],x['score']) )) # the corpus with tfidf scores\n",
    "\n",
    "def search(query, topN):\n",
    "    tokens = sc.parallelize(tokenize(query)).map(lambda x: (x,1) ).collectAsMap()\n",
    "    bcTokens = sc.broadcast(tokens)\n",
    "  \n",
    "    joined_tfidf = tfidf_RDD.map(lambda (k,v): (k,bcTokens.value.get(k,'-'),v) ).filter(lambda (a,b,c): b != '-' )\n",
    "  \n",
    "    scount = joined_tfidf.map(lambda a: a[2]).aggregateByKey((0,0),\n",
    "    (lambda acc, value: (acc[0] +value,acc[1]+1)),\n",
    "    (lambda acc1,acc2: (acc1[0]+acc2[0],acc1[1]+acc2[1])) )\n",
    "  \n",
    "    scores = scount.map(lambda (k,v): ( v[0]*v[1]/len(tokens), k) ).top(topN)\n",
    "  \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('Ink helps drive democracy in Asia The Kyrgyz Republic, a small, mountainous state of the former Soviet republic, is using invisible ink and ultraviolet readers in the country',5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source: https://www.linkedin.com/pulse/understanding-tf-idf-first-principle-computation-apache-asimadi/\n",
    "- And: https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6052175677058526/3537626382528910/5364082293869370/latest.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
